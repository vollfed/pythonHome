{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, TensorFlow!... finally\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "msg = tf.constant('Hello, TensorFlow!... finally')\n",
    "tf.print(msg)\n",
    "\n",
    "%matplotlib inline\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Review length: 57.75573832009506\n",
      "Standard Deviation: 25.0\n"
     ]
    }
   ],
   "source": [
    "n = ['id', 'date', 'name', 'text', 'typr', 'rep', 'rtw', 'faw', 'stcount', 'foll', 'frien', 'listcount']\n",
    "\n",
    "data_positive = pd.read_csv('positive.csv', sep=';', error_bad_lines=False, names=n, usecols=['text'])\n",
    "data_negative = pd.read_csv('negative.csv', sep=';', error_bad_lines=False, names=n, usecols=['text'])\n",
    "\n",
    "sample_size = min(data_positive.shape[0], data_negative.shape[0])\n",
    "raw_data = np.concatenate((data_positive['text'].values[:sample_size],\n",
    "                           data_negative['text'].values[:sample_size]), axis=0)\n",
    "labels = [1] * sample_size + [0] * sample_size\n",
    "\n",
    "#data_positive['text'][1243]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower().replace(\"ё\", \"е\")\n",
    "    #replace URL an NICKNAME\n",
    "    text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', text)\n",
    "    text = re.sub(r'@[^\\s]+', 'USER', text)\n",
    "    #delete words with letters-symbols and lang mix\n",
    "    text = re.sub(r'[\\W]+', ' ', text)\n",
    "    #delete predlogi\n",
    "    #text = re.sub(r'\\b[\\w]{,3}\\b', '', text)\n",
    "    text = re.sub(r'\\b[^не]{,3}\\b', ' ', text)\n",
    "    #delete tab symbols\n",
    "    text = re.sub(r'[\\s]+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "data_int = [preprocess_text(t) for t in raw_data]\n",
    "\n",
    "\n",
    "length = [len(i) for i in data_int]\n",
    "print(\"Average Review length:\", np.mean(length))\n",
    "print(\"Standard Deviation:\", round(np.std(length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z USER z ахахах ну ты ж ее стааарше меня мне нужна поддержка ты ж не t против кому если не тебе a\n",
      "  USER  ахахах  ну   ее  стааарше  меня  мне  нужна  поддержка   не  против  кому  если  не  тебе  \n"
     ]
    }
   ],
   "source": [
    "foo='z USER z ахахах ну ты ж ее стааарше меня мне нужна поддержка ты ж не t против кому если не тебе a'\n",
    "#foo=raw_data[1]\n",
    "\n",
    "i=98\n",
    "#print(raw_data[i])\n",
    "#print(data_int[i])\n",
    "print(foo)\n",
    "print(re.sub(r'\\b[^не]{,3}\\b', ' ', foo))\n",
    "\n",
    "#print([index.get(str,0) for str in data_int[i].split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_LENGTH = 50\n",
    "NUM = 10000\n",
    "NUM_TEST = 50000\n",
    "\n",
    "def get_sequences(data_p):\n",
    "    sequences = tokenizer.texts_to_sequences(data_p)\n",
    "    return pad_sequences(sequences, maxlen=SENTENCE_LENGTH)\n",
    "\n",
    "#we will use only NUM top used words as comp nerezinovi\n",
    "def get_my_sequences(comment_p):\n",
    "    result = [index.get(str,1) for str in comment_p.split(' ') if index.get(str,1) < NUM]\n",
    "    return result\n",
    "\n",
    "def vectorize(sequences, dimension = NUM):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM,oov_token=\"#\")\n",
    "tokenizer.fit_on_texts(data_int[NUM:])\n",
    "\n",
    "index = tokenizer.word_index\n",
    "\n",
    "#Change words to indexes from vocabulary that we sdelali\n",
    "data = [get_my_sequences(comment) for comment in data_int]\n",
    "\n",
    "#Change to 0-1 index\n",
    "data = vectorize(data)\n",
    "targets = np.array(labels).astype(\"float32\")\n",
    "        \n",
    "#this is keras train/test it is with shuffle\n",
    "train_x, test_x, train_y, test_y = train_test_split(data, labels, test_size=0.2, random_state=2)\n",
    "\n",
    "#test_x = data[:NUM_TEST]\n",
    "#test_y = targets[:NUM_TEST]\n",
    "\n",
    "#train_x = data[NUM_TEST:]\n",
    "#train_y = targets[NUM_TEST:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13866\n",
      "2892\n",
      "170145\n",
      "data:_____\n",
      "USER углу сидит погибает голода еще порции взяли хотя уже жрать не хотим\n",
      "[1, 5005, 759, 163604, 6285, 10, 16649, 1653, 97, 11, 889, 3, 2216]\n",
      "[1, 5005, 759, 6285, 10, 1653, 97, 11, 889, 3, 2216]\n",
      "[0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "i=3\n",
    "ttt1 = [index.get(str,1) for str in data_int[i].split(' ')]\n",
    "print(index.get('школота'))\n",
    "print(index.get('поверь'))\n",
    "print(len(index))\n",
    "\n",
    "print('data:_____')\n",
    "print(data_int[i])\n",
    "print(ttt1)\n",
    "print(get_my_sequences(data_int[i]))\n",
    "print(data[i][0:60])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-3 is some magic.# replace when word not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                500050    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 505,201\n",
      "Trainable params: 505,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "# Input - Layer\n",
    "model.add(layers.Dense(50, activation = \"relu\", input_shape=(10000, )))\n",
    "# Hidden - Layers\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "# Output- Layer\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    " optimizer = \"adam\",\n",
    " loss = \"binary_crossentropy\",\n",
    " metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 179076 samples, validate on 44770 samples\n",
      "Epoch 1/2\n",
      "179076/179076 [==============================] - 60s 336us/step - loss: 0.5735 - accuracy: 0.6967 - val_loss: 0.5330 - val_accuracy: 0.7306\n",
      "Epoch 2/2\n",
      "179076/179076 [==============================] - 64s 360us/step - loss: 0.4992 - accuracy: 0.7544 - val_loss: 0.5285 - val_accuracy: 0.7341\n"
     ]
    }
   ],
   "source": [
    "results = model.fit(\n",
    " train_x, train_y,\n",
    " epochs= 2,\n",
    " batch_size = 500,\n",
    " validation_data = (test_x, test_y)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7323542535305023\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(results.history[\"val_accuracy\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "прекрасный наполненный смыслом и радостью фильм понравился очень\n",
      "этот фильм полный шлак посмотрел самый худший фильм видел оценка минус один нуля\n",
      "\n",
      "Get index \n",
      "\n",
      "[[1405, 1, 8683, 2385, 179, 936, 18]]\n",
      "[[68, 179, 1084, 784, 182, 6773, 179, 493, 5039, 1132, 82]]\n"
     ]
    }
   ],
   "source": [
    "testPositive = \"прекрасный наполненный смыслом и радостью фильм понравился очень\"\n",
    "testNegative = \"этот фильм полный шлак посмотрел самый худший фильм видел оценка минус один нуля\"\n",
    "\n",
    "print(testPositive)\n",
    "print(testNegative)\n",
    "\n",
    "testPositiveArr = [1]\n",
    "testNegativeArr = [1]\n",
    "\n",
    "print(\"\\n\"+\"Get index \"+\"\\n\")\n",
    "\n",
    "testPositiveArr[0] = get_my_sequences(testPositive)\n",
    "testNegativeArr[0] = get_my_sequences(testNegative)\n",
    "\n",
    "print(testPositiveArr)\n",
    "print(testNegativeArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 0.8635264\n",
      "Negative: 0.051291257\n"
     ]
    }
   ],
   "source": [
    "testPositiveArrINDX = vectorize(testPositiveArr)\n",
    "testNegativeArrINDX = vectorize(testNegativeArr)\n",
    "\n",
    "predictPositive = model.predict(testPositiveArrINDX)\n",
    "predictNegative = model.predict(testNegativeArrINDX)\n",
    "\n",
    "print('Positive: ' + str(predictPositive[0][0]))\n",
    "print('Negative: ' + str(predictNegative[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('trainedModel.hdf5')\n",
    "df = pd.DataFrame.from_dict(index, orient=\"index\")\n",
    "df.to_csv(\"commentDict.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
